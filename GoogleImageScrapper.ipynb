{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odi2vIMHC3Rm"
      },
      "source": [
        "# Welcome to google image scraping, multi core scrapping directly online\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y chromium-chromedriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip xvfb libxi6 libgconf-2-4\n",
        "!apt-get install -y google-chrome-stable\n",
        "!wget https://chromedriver.storage.googleapis.com/134.0.6366.121/chromedriver_linux64.zip\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "!pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nLfllpTD7Tvu",
        "outputId": "fa85ec55-7f14-4aa6-9793-08fae7b91b8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [112 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [38.5 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,678 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,763 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Fetched 24.4 MB in 10s (2,450 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "0 upgraded, 8 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 30.2 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.2 MB in 2s (17.2 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Selecting previously unselected package udev.\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126638 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s19ucTii_wYb"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "#Sample queries added\n",
        "import sys\n",
        "import csv\n",
        "import time\n",
        "import urllib.parse\n",
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import StaleElementReferenceException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "service = Service(ChromeDriverManager().install())\n",
        "def parse_imgurl(href):\n",
        "    \"\"\"\n",
        "    Extract the 'imgurl' parameter from a Google '/imgres?' link.\n",
        "    Returns the high-resolution image URL if found, otherwise None.\n",
        "    \"\"\"\n",
        "    import urllib.parse\n",
        "    parsed = urllib.parse.urlparse(href)\n",
        "    qs = urllib.parse.parse_qs(parsed.query)\n",
        "    return qs.get(\"imgurl\", [None])[0]\n",
        "\n",
        "def append_to_csv(data_list, csv_filename=\"images_data.csv\"):\n",
        "    \"\"\"\n",
        "    Appends rows to 'csv_filename'. Each row has columns: url, alt.\n",
        "    If file doesn't exist, it is created, but existing data is preserved.\n",
        "    \"\"\"\n",
        "    import csv\n",
        "    file_exists = os.path.isfile(csv_filename)\n",
        "    with open(csv_filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        fieldnames = [\"url\", \"alt\"]\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "\n",
        "        # If file is newly created, write the header\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "\n",
        "        for row in data_list:\n",
        "            writer.writerow({\"url\": row[\"url\"], \"alt\": row[\"alt\"]})\n",
        "\n",
        "def scrape_google_images(query, max_links=10):\n",
        "    \"\"\"\n",
        "    Runs the Google Images scraping for a single query.\n",
        "    Returns a list of dicts: [{'url': ..., 'alt': ...}, ...]\n",
        "    \"\"\"\n",
        "    # Setup headless Chrome\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\n",
        "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "\n",
        "    # If multiple processes try to install or update ChromeDriver simultaneously,\n",
        "    # you might see \"File is not a zip file\" or \"tuple index out of range\".\n",
        "    # One workaround is to install ChromeDriver once outside the processes,\n",
        "    # or catch these exceptions.\n",
        "\n",
        "    driver = webdriver.Chrome( options=chrome_options)\n",
        "\n",
        "    image_data = []\n",
        "    seen_urls = set()\n",
        "\n",
        "    try:\n",
        "        # 1. Navigate to Google Images\n",
        "        search_url = f\"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}\"\n",
        "        driver.get(search_url)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 2. Scroll to load more containers\n",
        "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # 3. Find containers with class 'ivg-i'\n",
        "        containers = driver.find_elements(By.CSS_SELECTOR, \"div.ivg-i\")\n",
        "        print(f\"[{query}] Found {len(containers)} containers.\")\n",
        "\n",
        "        actions = ActionChains(driver)\n",
        "\n",
        "        for i, container in enumerate(containers):\n",
        "            if len(image_data) >= max_links:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Hover over the container\n",
        "                actions.move_to_element(container).perform()\n",
        "                k=1\n",
        "\n",
        "                # Find anchor with href^='/imgres?'\n",
        "                anchor = container.find_element(By.CSS_SELECTOR, \"a[href^='/imgres?']\")\n",
        "                href = anchor.get_attribute(\"href\")\n",
        "                if href and \"imgurl=\" in href:\n",
        "                    high_res_url = parse_imgurl(href)\n",
        "                    if high_res_url and high_res_url not in seen_urls:\n",
        "                        seen_urls.add(high_res_url)\n",
        "                        # Attempt to get alt text from child <img>\n",
        "                        try:\n",
        "                            child_img = anchor.find_element(By.TAG_NAME, \"img\")\n",
        "                            alt_text = child_img.get_attribute(\"alt\") or \"\"\n",
        "                        except:\n",
        "                            alt_text = \"\"\n",
        "\n",
        "                        image_data.append({\"url\": high_res_url, \"alt\": alt_text})\n",
        "                        print(f\"[{query}] Container #{i+1}: {high_res_url[:80]}... Alt: {alt_text}\")\n",
        "            except StaleElementReferenceException:\n",
        "                pass\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return image_data\n",
        "\n",
        "def process_query(query, max_links=10, csv_file=\"images_data.csv\"):\n",
        "    \"\"\"\n",
        "    Worker function to scrape images for a single query and append them to a CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = scrape_google_images(query, max_links=max_links)\n",
        "        append_to_csv(data, csv_filename=csv_file)\n",
        "        return f\"Scraped {len(data)} images for '{query}'.\"\n",
        "    except Exception as e:\n",
        "        # If you get \"File is not a zip file\" or others, handle them here\n",
        "        return f\"Error scraping '{query}': {e}\"\n",
        "\n",
        "def main():\n",
        "\n",
        "    queries = [\n",
        "        \"bose headphones\",\n",
        "        \"sony headphones\",\n",
        "        \"jbl headphones\",\n",
        "    ]\n",
        "    max_links = 1000\n",
        "    csv_name = \"images.csv\"\n",
        "    # We'll run them in parallel using ProcessPoolExecutor\n",
        "    import concurrent.futures\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        # Submit tasks\n",
        "        futures = {executor.submit(process_query, q, max_links,csv_name): q for q in queries}\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            q = futures[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                print(f\"[{q}] {result}\")\n",
        "            except Exception as exc:\n",
        "                print(f\"[{q}] generated an exception: {exc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "OPGni6fuvoTj"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "import csv\n",
        "import time\n",
        "import urllib.parse\n",
        "import os\n",
        "import concurrent.futures\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "service = Service(ChromeDriverManager().install())\n",
        "def parse_imgurl(href):\n",
        "    \"\"\"\n",
        "    Extract the 'imgurl' parameter from a Google '/imgres?' link.\n",
        "    Returns the high-resolution image URL if found, otherwise None.\n",
        "    \"\"\"\n",
        "    import urllib.parse\n",
        "    parsed = urllib.parse.urlparse(href)\n",
        "    qs = urllib.parse.parse_qs(parsed.query)\n",
        "    return qs.get(\"imgurl\", [None])[0]\n",
        "\n",
        "def append_to_csv(data_list, csv_filename=\"images_data.csv\"):\n",
        "    \"\"\"\n",
        "    Appends rows to 'csv_filename'. Each row has columns: url, alt.\n",
        "    If file doesn't exist, it is created, but existing data is preserved.\n",
        "    \"\"\"\n",
        "    import csv\n",
        "    file_exists = os.path.isfile(csv_filename)\n",
        "    with open(csv_filename, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        fieldnames = [\"url\", \"alt\"]\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "\n",
        "        # If file is newly created, write the header\n",
        "        if not file_exists:\n",
        "            writer.writeheader()\n",
        "\n",
        "        for row in data_list:\n",
        "            writer.writerow({\"url\": row[\"url\"], \"alt\": row[\"alt\"]})\n",
        "\n",
        "def scrape_google_images(query, max_links=10):\n",
        "    \"\"\"\n",
        "    Runs the Google Images scraping for a single query.\n",
        "    Returns a list of dicts: [{'url': ..., 'alt': ...}, ...]\n",
        "    \"\"\"\n",
        "    # Setup headless Chrome\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\n",
        "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "\n",
        "    # If multiple processes try to install or update ChromeDriver simultaneously,\n",
        "    # you might see \"File is not a zip file\" or \"tuple index out of range\".\n",
        "    # One workaround is to install ChromeDriver once outside the processes,\n",
        "    # or catch these exceptions.\n",
        "\n",
        "    driver = webdriver.Chrome( options=chrome_options)\n",
        "\n",
        "    image_data = []\n",
        "    seen_urls = set()\n",
        "\n",
        "    try:\n",
        "        # 1. Navigate to Google Images\n",
        "        search_url = f\"https://www.google.com/search?tbm=isch&q={urllib.parse.quote(query)}\"\n",
        "        driver.get(search_url)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # 2. Scroll to load more containers\n",
        "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "        time.sleep(1)\n",
        "\n",
        "        # 3. Find containers with class 'ivg-i'\n",
        "        containers = driver.find_elements(By.CSS_SELECTOR, \"div.ivg-i\")\n",
        "        print(f\"[{query}] Found {len(containers)} containers.\")\n",
        "\n",
        "        actions = ActionChains(driver)\n",
        "\n",
        "        for i, container in enumerate(containers):\n",
        "            if len(image_data) >= max_links:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # Hover over the container\n",
        "                actions.move_to_element(container).perform()\n",
        "                k=1\n",
        "\n",
        "                # Find anchor with href^='/imgres?'\n",
        "                anchor = container.find_element(By.CSS_SELECTOR, \"a[href^='/imgres?']\")\n",
        "                href = anchor.get_attribute(\"href\")\n",
        "                if href and \"imgurl=\" in href:\n",
        "                    high_res_url = parse_imgurl(href)\n",
        "                    if high_res_url and high_res_url not in seen_urls:\n",
        "                        seen_urls.add(high_res_url)\n",
        "                        # Attempt to get alt text from child <img>\n",
        "                        try:\n",
        "                            child_img = anchor.find_element(By.TAG_NAME, \"img\")\n",
        "                            alt_text = child_img.get_attribute(\"alt\") or \"\"\n",
        "                        except:\n",
        "                            alt_text = \"\"\n",
        "\n",
        "                        image_data.append({\"url\": high_res_url, \"alt\": alt_text})\n",
        "                        print(f\"[{query}] Container #{i+1}: {high_res_url[:80]}... Alt: {alt_text}\")\n",
        "            except StaleElementReferenceException:\n",
        "                pass\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return image_data\n",
        "\n",
        "def process_query(query, max_links=10, csv_file=\"images_data.csv\"):\n",
        "    \"\"\"\n",
        "    Worker function to scrape images for a single query and append them to a CSV.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = scrape_google_images(query, max_links=max_links)\n",
        "        append_to_csv(data, csv_filename=csv_file)\n",
        "        return f\"Scraped {len(data)} images for '{query}'.\"\n",
        "    except Exception as e:\n",
        "        # If you get \"File is not a zip file\" or others, handle them here\n",
        "        return f\"Error scraping '{query}': {e}\"\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python script.py [<csv_name>] <query1> [<query2> ...] [<max_links>]\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    queries = sys.argv[2:-1]  # All arguments except the last one (if it's a number)\n",
        "    try:\n",
        "        max_links = int(sys.argv[-1]) if sys.argv[-1].isdigit() else 100\n",
        "    except ValueError:\n",
        "        max_links = 100\n",
        "    csv_name = sys.argv[1] if len(sys.argv) > 1 else \"images_data.csv\"\n",
        "\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = {executor.submit(process_query, q, max_links): q for q in queries}\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            q = futures[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                print(f\"[{q}] {result}\")\n",
        "            except Exception as exc:\n",
        "                print(f\"[{q}] Exception: {exc}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r13YcYhtBNiU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tensor2Tensor Intro",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}